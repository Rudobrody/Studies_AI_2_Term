{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b825e1aa",
   "metadata": {},
   "source": [
    "# 0.0 Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f2f20",
   "metadata": {},
   "source": [
    "Wykorzystując przekazaną implementację ANFIS, naucz system go tabliczki mnożenia. Zacznij od rozmiaru 2x2 i zwiększaj w obu kierunkach z krokiem 1 (kolejno 2x2, 3x3, 4x4, 5x5). Mierz czas jaki jest potrzebny na nauczenie systemu w zależności od wymiarów tabliczki. Zrób wykres wymiaru tabliczki od czasu uczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85808e6c",
   "metadata": {},
   "source": [
    "# 0.1 Theory about ANFIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1884f2",
   "metadata": {},
   "source": [
    "What is **Adaptive Neuro-Fuzzy Inference System** ?  \n",
    "  \n",
    "ANFIS is a hybrid model that combines the strength of Artificial Neural Network and Fuzzy Inference Systems (FIS). \n",
    "\n",
    "**What are strengths of Fuzzy Inference System?**  \n",
    "  \n",
    "* Interpretability: it uses IF-THEN rules\n",
    "* We can directly encode knowledge from a human expert into the rule base.\n",
    "\n",
    "**Weaknesses**\n",
    "\n",
    "* Tuning - manually defining the exact shape of the fuzzy sets like \"what does 'high temperature' mean?\". Difficulties with trial and error process.   \n",
    "\n",
    "**What are strengths of Artificial Neural Network?**  \n",
    "\n",
    "* Learning capability\n",
    "* It can adapt to new, unseen data\n",
    "\n",
    "**Weaknesses**  \n",
    "\n",
    "* Black box nature\n",
    "\n",
    "**What is core of Adaptive Nuero-Fuzzy Inference System?**  \n",
    "  \n",
    "The aim was to create a system that can learn from data (like a neural network) but whose behavior can be undestood and intrepreted (like a fuzzy system) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f25a4",
   "metadata": {},
   "source": [
    "# 0.2 Architecure of Adaptive Neuro-Fuzzy Inference System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2650ca0",
   "metadata": {},
   "source": [
    "1. **Fuzzification Layer**   \n",
    "Determines the degree to which each input belongs to a fuzzy set. Every node in this layer is an adaptive node with a function (membership function)  \n",
    "2. **Rule Layer**  \n",
    "Combines the membership degress from the previous layer to get the \"firinig strength\" or \"weight\" of each rule.  Each node in this layer represents a single fuzzy rule. It takes the output from Layer 1 that corresponds to it's rule's antecedents, usually by multiplying them.  \n",
    "\n",
    "Example: w1 = membersip(temp is warm) * membership (humidity is medium). The w1 is the firing strength of rule 1 \n",
    "    \n",
    "3. **Normalization layer**  \n",
    "To normalize the firing strengths of the rules. Each node takes the firing strength of a rule from Layer 2 and divides it by the sum of all firing strength.  \n",
    "\n",
    "4. **Defuzzifaction layer**  \n",
    "To compute the output of each rule. Each node in this layer calculates an output based on the consequent part of the rule. In a Takagi-Sugeno, this is a linear function of the inputs  \n",
    "  \n",
    "5. **Output layer**  \n",
    "To compute the final, overall input. Single node that sums up the outputs from all the nodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151e10",
   "metadata": {},
   "source": [
    "**Forward and backward pass**  \n",
    "Input data is fed forward trough the network. The consequent parameters are learned using a fast method like Least Square Estimation. The error - the difference between the ANFIS output and the actual target value is calculated and propagated backward through the network. Parameters are updated using Gradient Descent.  \n",
    "\n",
    "Few words about Least Square Estimation and Gradient Descent.  \n",
    "  \n",
    "Why we use Least Square Estimation? The ANFIS output is a linear combination of the consequent parameters. LSE is the analytical, one-step solution:  \n",
    "* it is fast, because of using matrix operations  \n",
    "* it is opitmal, becauce it is mathematically guaranteed to find the global minimum for this linear sub-problem. It doesn't get stuck in local minima.  \n",
    "* it is deterministic, it will give the exact same solution every time for te same data.   \n",
    "\n",
    "The relation between the final output error adn the premise parameters is highly non-linear. There is no direct, a nalytical way to so solver for the best premise parameters in one step like we can with LSE. We must use iterative optimization. Standard **Gradient Descent** can be slow, gest stuck in local minima and is sensitive to the learning rate. There are alterantives like:  \n",
    "* Stochastic Gradient Descent: Instead of calculating the gradient over the entire dataset at each step, we using mini-batch of samples\n",
    "* Momentum: This method adds a fraction of the previous update vector to the current one. So it doesn't stop in local minima as easily. \n",
    "* Adaptive Moment Estimation: Often it is the default, go-to optimizer in deep learning today. It combines **Momentum** and **RMSprop**. It computes adaptive learning rates for each parameter, meaning it can take large step for some parameters and small for anothers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
